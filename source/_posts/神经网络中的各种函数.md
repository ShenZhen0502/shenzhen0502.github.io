---
title: 神经网络中的各种函数
date: 2024-02-28 16:04:13
tags:
categories: 深度学习基础知识

---

# 激活函数

#### SoftMax



# 损失函数

#### Cross Entropy Error(交叉熵误差)

数学表达式为：
$$
L=-\sum_{k}{t_{k}log_{y_{k}}}         ..................其中k代表类别
$$
这是一个用于多分类的损失函数。$t_k$代表第k个类别的标签值，$y_k$代表第k个类别的网络输出值。$t_k$中只有正确解标签值为1，其余均为0，这里为了便于理解我们假设$t_k$的值为$[0,0,1,0,0,0]$。假设我们的神经网络$y_k$输出的值为$[0.1,0.2,0.9,0.02,0.03,0.04]$，那么交叉熵误差就可以表示为：
$$
L=-(0*log_{0.1}+0*log_{0.2}+1*log_{0.9}+0*log_{0.02}+0*log_{0.03}+0*log_{0.04})=-log_{0.9}
$$
可以看出交叉熵误差只会计算当k项为1时候的误差，

# 其他

